# Customer Churn Prediction & Retention Pipeline

## ğŸ“Œ Project Overview
This project implements an **end-to-end customer churn prediction pipeline** for a telecom company using the IBM Telco Customer Churn dataset. It covers:

- Infrastructure provisioning with **Terraform**
- Configuration automation with **Ansible**
- ETL and data processing with **AWS Glue & PySpark**
- Data storage and partitioning with **S3**
- CI/CD deployment with **GitHub Actions**
- Real-time job monitoring using **CloudWatch**
- IAM-based access management
- ML model for churn prediction with ML-ready output

---

## ğŸ§¬ Dataset Path in S3
```
s3://telco-churn-data-hauwa/
â”œâ”€â”€ raw/
â”‚   â””â”€â”€ WA_Fn-UseC_-Telco-Customer-Churn.csv
â”œâ”€â”€ processed/
â”‚   â””â”€â”€ *.parquet
â””â”€â”€ ml-ready/
    â”œâ”€â”€ Churn=Yes/
    â””â”€â”€ Churn=No/
```

---

## âš™ï¸ Technologies Used
- **AWS S3** - Data lake for raw, processed, and ML-ready data
- **AWS Glue** - ETL pipeline for cleaning, transforming, partitioning
- **Terraform** - Infrastructure provisioning
- **Ansible** - Post-deploy automation
- **GitHub Actions** - CI/CD automation for IaC deployments
- **CloudWatch** - Monitoring Glue job runs
- **IAM** - Secure access management
- **PySpark** - Spark data processing
- **Python** - ML prediction script

---

## ğŸ” ETL Workflow
1. Upload raw CSV to `s3://telco-churn-data-hauwa/raw/`
2. Run Glue ETL job:
   - Cleans & drops nulls
   - Converts to Parquet
   - Saves to `processed/`
3. ML-ready transformation:
   - Partitioned into `Churn=Yes/No` folders
   - Output stored in `ml-ready/`

---

## ğŸ“¸ Screenshots & Architecture

### `Architectur`
- ğŸ“Œ Description: End-to-end architecture of the pipeline
![Architecture Diagram](images/architecture.png)


### `etl-flow.png`
- ğŸ“Œ Description: ETL steps from raw to ML-ready
- ğŸ“· Location: `docs/images/etl-flow.png`
- ğŸ›  How to: Draw raw âœ processed âœ ml-ready flow

### `s3-output.png`
- ğŸ“Œ Description: Screenshot of S3 folders with ML-ready churn partitions
![ML-ready Screenshort](images/ml_image.png)

```

---

## âœ… Deployment Steps
1. **Upload Dataset** to S3 `raw/`
2. **Terraform Apply** to provision infra
3. **Ansible Playbook** (if needed) to configure resources
4. **Run GitHub Actions CI/CD** to trigger Glue Job
5. **Verify in Glue Job Run & S3 Output**
6. **Run ML Script** for churn prediction on processed data

---

## ğŸ“© Author
**Hauwa Kulu Njidda**
- ğŸ“§ hauwa.dbtech@gmail.com
- ğŸ”— [LinkedIn](https://www.linkedin.com/in/hauwa-kulu-njidda)

---
